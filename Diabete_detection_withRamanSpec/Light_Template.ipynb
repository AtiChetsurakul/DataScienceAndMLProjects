{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from BaselineRemoval import BaselineRemoval\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# from torch.utils.data import TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATALABEL = ['ear lobe','inner arm','thumb nail','median cubital vein']\n",
    "WATCH_PROB_PREDICT = False ## if you want to observe the probability of each choice\n",
    "PLT = '/root/projects/ProjectAug22/CP/Diabete_detection_withRamanSpec/dataset/' ## change to the path to dataset\n",
    "PLT_mac = '/Volumes/ExternalSSDForMac/AITLecture/CSforDSAIAUG22/_CP_project/DataScienceAndMLProjects/Diabete_detection_withRamanSpec/dataset'\n",
    "\n",
    "SMOOTH = False\n",
    "\n",
    "SEED = 25811243\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import spectrumloader as spl\n",
    "from src import get_torch_loader as tlg\n",
    "# from src.modeltorch_template import NeuralNet,RamConv1d,RamLSTM,RamConv1d_bnmx,IntANN,IntRamLSTM,RamConv1d_mx\n",
    "from src import trainResult_show as mtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard,data_all = spl.read_file('dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs,ys,_ = spl.cut_tonumpy(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(Xs) == 4, 'DATA error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Train test split (with same seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainall,_,y_trainall,_ = spl.split_train_test(Xs,ys)\n",
    "y_train0,y_train1,y_train2,y_train3 = y_trainall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA [here](https://github.com/AtiChetsurakul/DataScienceAndMLProjects/blob/main/Diabete_detection_withRamanSpec/datawatcher.ipynb)\n",
    "- To shorten the file, eda not inculded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. spectrum tranformation \n",
    "- incuding\n",
    "    - Fluoresence removal\n",
    "    - Correcting baseline\n",
    "    - Normalization\n",
    "## ORDER OF DATA ARE\n",
    ">(0) ear lobe\n",
    "\n",
    ">(1) inner arm \n",
    "\n",
    ">(2) thumb nail \n",
    "\n",
    ">(3) median cubital vein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_VR = spl.fluoresence_removal(X_trainall)\n",
    "if SMOOTH:\n",
    "    X_train_VR = spl.savitzky_smooth(X_train_VR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train0_std,X_train1_std,X_train2_std,X_train3_std,normalizer = spl.seting_normalized_fuoresence_smoothing(True,True,X_train_VR)\n",
    "\n",
    "assert len(normalizer) == 4, 'Normalizer is not here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check by plot\n",
    "plt.figure(figsize=(5,2))\n",
    "plt.plot(X_train_VR[0][0])\n",
    "plt.title('spectrum with preprocess but without normalize')\n",
    "plt.show()\n",
    "plt.figure(figsize=(5,2))\n",
    "plt.title('spectrum with preprocess and with normalize')\n",
    "plt.plot(X_train1_std[0])\n",
    "plt.plot(X_train1_std[7])\n",
    "plt.plot(X_train1_std[2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. apply data to pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_earloop = TensorDataset(torch.FloatTensor(X_train0_std),torch.LongTensor(y_train0))\n",
    "ds_inarm = TensorDataset(torch.FloatTensor(X_train1_std),torch.LongTensor(y_train1))\n",
    "ds_thumbnail = TensorDataset(torch.FloatTensor(X_train2_std),torch.LongTensor(y_train2))\n",
    "ds_vain = TensorDataset(torch.FloatTensor(X_train3_std),torch.LongTensor(y_train3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 12\n",
    "val_size = 4\n",
    "batch_size=12\n",
    "valbatch_size=4\n",
    "train_loader_all,val_loader_all = tlg.shuffleloadertorch(ds_earloop,ds_inarm,ds_thumbnail,ds_vain,SEED,train_size,val_size,batch_size,valbatch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. modeling\n",
    "- ### 6.simple\n",
    "    - ### ANN\n",
    "# Template to Run ANN model\n",
    "- ### to use change the model name and try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ADD YOUR MODEL HERE'''\n",
    "\n",
    "raise(NotImplementedError)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE model_for_vain = NeuralNet(1000, 14, 2).to(device) \n",
    "\n",
    "model_for_earloop = \"MODEL NAME\"(1000, 14, 2).to(device)\n",
    "model_for_inarm = \"MODEL\"(1000, 14, 2).to(device)\n",
    "model_for_thumbnail = \"MODEL NAME\"(1000, 14, 2).to(device)\n",
    "model_for_vain = \"MODEL NAME\"(1000, 14, 2).to(device)\n",
    "\n",
    "\n",
    "criterion_for_earloop = nn.CrossEntropyLoss()   \n",
    "optimizer_for_earloop = torch.optim.Adam(model_for_earloop.parameters(), lr=0.0001)  \n",
    "\n",
    "criterion_for_inarm= nn.CrossEntropyLoss()   \n",
    "optimizer_for_inarm = torch.optim.Adam(model_for_inarm.parameters(), lr=0.0001)  \n",
    "\n",
    "criterion_for_thumbnail= nn.CrossEntropyLoss()   \n",
    "optimizer_for_thumbnail = torch.optim.Adam(model_for_thumbnail.parameters(), lr=0.0001)\n",
    "\n",
    "criterion_for_vain = nn.CrossEntropyLoss()   \n",
    "optimizer_for_vain = torch.optim.Adam(model_for_vain.parameters(), lr=0.0001)  \n",
    "\n",
    "annsim_models = [model_for_earloop,model_for_inarm,model_for_thumbnail,model_for_vain]\n",
    "annsim_criterions = [criterion_for_earloop,criterion_for_inarm,criterion_for_thumbnail,criterion_for_vain]\n",
    "annsim_optims = [optimizer_for_earloop,optimizer_for_inarm,optimizer_for_thumbnail,optimizer_for_vain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annsim_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = train_size/batch_size  #for printing purpose\n",
    "num_epochs = 5000\n",
    "train_loss_each_location = []\n",
    "val_loss_each_location = []\n",
    "val_acc_each_location = []\n",
    "for model,criterion,optimizer,each_train_loader,each_val_loader,datasetlabel in zip(annsim_models,annsim_criterions,annsim_optims,train_loader_all,val_loader_all,DATALABEL):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (spec, y) in enumerate(each_train_loader):  \n",
    "            \n",
    "\n",
    "            spec = spec.to(device)\n",
    "            y = y.to(device)\n",
    "                    \n",
    "            # Forward pass\n",
    "            outputs = model(spec)\n",
    "            loss = criterion(outputs, y)\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i) % 20 == 0:\n",
    "                sys.stdout.write('\\rEpoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                    .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "        if (epoch+1) % 49 == 0:\n",
    "            with torch.no_grad():\n",
    "                total_val_corr = 0\n",
    "                for (val_spec, val_label) in each_val_loader:\n",
    "                    val_spec = val_spec.to(device)\n",
    "                    val_label = val_label.to(device)\n",
    "                    val_yhat = model(val_spec)\n",
    "                    val_loss = criterion(val_yhat, val_label)\n",
    "\n",
    "                    train_losses.append(loss.cpu().detach().numpy())\n",
    "                    val_losses.append(val_loss.cpu().detach().numpy())\n",
    "\n",
    "                    val_predicted = torch.max(val_yhat, 1)[1]\n",
    "                    total_val_corr += (val_predicted == val_label).sum()\n",
    "                val_acc = (total_val_corr * 100) / val_size\n",
    "                print(f\"    ++++++Validation++++++  Loss: {val_loss:.2f} - Acc: {val_acc:.2f}\",end = ' ')\n",
    "\n",
    "    print(f'\\n END OF MODEL for {datasetlabel} with val acc = {val_acc}')\n",
    "    train_loss_each_location.append(train_losses)\n",
    "    val_loss_each_location.append(val_losses)\n",
    "    val_acc_each_location.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtp.loss_plot_sub(train_loss_each_location,val_loss_each_location,val_acc_each_location,DATALABEL,'simple ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 6.simple\n",
    "    - ### CNN\n",
    "# Template FOR test your CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ADD YOUR MODEL HERE'''\n",
    "\n",
    "raise(NotImplementedError)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE modelcnn_for_earloop = RamConv1d().to(device)\n",
    "\n",
    "modelcnn_for_earloop = \"MODEL NAME\"().to(device)\n",
    "modelcnn_for_inarm = \"MODEL NAME\"().to(device)\n",
    "modelcnn_for_thumbnail = \"MODEL NAME\"().to(device)\n",
    "modelcnn_for_vain = \"MODEL NAME\"().to(device)\n",
    "\n",
    "\n",
    "criterion_for_earloop = nn.CrossEntropyLoss()   \n",
    "optimizer_for_earloop = torch.optim.Adam(modelcnn_for_earloop.parameters(), lr=0.0001)  \n",
    "\n",
    "criterion_for_inarm= nn.CrossEntropyLoss()   \n",
    "optimizer_for_inarm = torch.optim.Adam(modelcnn_for_inarm.parameters(), lr=0.0001)  \n",
    "\n",
    "criterion_for_thumbnail= nn.CrossEntropyLoss()   \n",
    "optimizer_for_thumbnail = torch.optim.Adam(modelcnn_for_thumbnail.parameters(), lr=0.0001)\n",
    "\n",
    "criterion_for_vain = nn.CrossEntropyLoss()   \n",
    "optimizer_for_vain = torch.optim.Adam(modelcnn_for_vain.parameters(), lr=0.0001)  \n",
    "\n",
    "cnnsim_models = [modelcnn_for_earloop,modelcnn_for_inarm,modelcnn_for_thumbnail,modelcnn_for_vain]\n",
    "cnnsim_criterions = [criterion_for_earloop,criterion_for_inarm,criterion_for_thumbnail,criterion_for_vain]\n",
    "cnnsim_optims = [optimizer_for_earloop,optimizer_for_inarm,optimizer_for_thumbnail,optimizer_for_vain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnsim_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = train_size/batch_size  #for printing purpose\n",
    "num_epochs = 5000\n",
    "\n",
    "train_loss_each_location = []\n",
    "val_loss_each_location = []\n",
    "val_acc_each_location = []\n",
    "\n",
    "for model,criterion,optimizer,each_train_loader,each_val_loader,datasetlabel in zip(cnnsim_models,cnnsim_criterions,cnnsim_optims,train_loader_all,val_loader_all,DATALABEL):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (spec, y) in enumerate(each_train_loader):  \n",
    "            \n",
    "\n",
    "            spec = spec.reshape(spec.shape[0],1,-1).to(device)\n",
    "            y = y.to(device)\n",
    "                    \n",
    "            # Forward pass\n",
    "            outputs = model(spec)\n",
    "            loss = criterion(outputs, y)  \n",
    "        \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i) % 20 == 0:\n",
    "                sys.stdout.write('\\rEpoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                    .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "        if (epoch+1) % 49 == 0:\n",
    "            with torch.no_grad():\n",
    "                total_val_corr = 0\n",
    "                for (val_spec, val_label) in each_val_loader:\n",
    "                    val_spec = val_spec.reshape(val_spec.shape[0],1,-1).to(device)\n",
    "                    val_label = val_label.to(device)\n",
    "                    val_yhat = model(val_spec)\n",
    "                    val_loss = criterion(val_yhat, val_label)\n",
    "\n",
    "                    train_losses.append(loss.cpu().detach().numpy())\n",
    "                    val_losses.append(val_loss.cpu().detach().numpy())\n",
    "\n",
    "                    val_predicted = torch.max(val_yhat, 1)[1]\n",
    "                    total_val_corr += (val_predicted == val_label).sum()\n",
    "                val_acc = (total_val_corr * 100) / val_size\n",
    "                print(f\"    ++++++Validation++++++  Loss: {val_loss:.2f} - Acc: {val_acc:.2f}\",end = ' ')\n",
    "    print(f'\\n END OF MODEL for {datasetlabel} with val acc = {val_acc}')\n",
    "    train_loss_each_location.append(train_losses)\n",
    "    val_loss_each_location.append(val_losses)\n",
    "    val_acc_each_location.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WATCH_PROB_PREDICT:\n",
    "    with torch.no_grad():\n",
    "        print('trainpred ',cnnsim_models[-1](spec),'\\n',y,'\\n val\\n',cnnsim_models[-1](val_spec),val_label)\n",
    "\n",
    "mtp.loss_plot_sub(train_loss_each_location,val_loss_each_location,val_acc_each_location,DATALABEL,'simple CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 6.simple\n",
    "    - RNN\n",
    "- ADD YOUR Try here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/ExternalSSDForMac/AITLecture/CSforDSAIAUG22/_CP_project/DataScienceAndMLProjects/Diabete_detection_withRamanSpec/Light_Template.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/ExternalSSDForMac/AITLecture/CSforDSAIAUG22/_CP_project/DataScienceAndMLProjects/Diabete_detection_withRamanSpec/Light_Template.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m'''ADD YOUR MODEL HERE'''\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Volumes/ExternalSSDForMac/AITLecture/CSforDSAIAUG22/_CP_project/DataScienceAndMLProjects/Diabete_detection_withRamanSpec/Light_Template.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mraise\u001b[39;00m(\u001b[39mNotImplementedError\u001b[39;00m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''ADD YOUR MODEL HERE'''\n",
    "\n",
    "raise(NotImplementedError)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAM PLE model_RNN_for_earloop = RamLSTM().to(device)\n",
    "\n",
    "model_RNN_for_earloop = \"MODEL NAME\"().to(device)\n",
    "model_RNN_for_inarm = \"MODEL NAME\"().to(device)\n",
    "model_RNN_for_thumbnail = \"MODEL NAME\"().to(device)\n",
    "model_RNN_for_vain = \"MODEL NAME\"().to(device)\n",
    "\n",
    "\n",
    "criterion_for_earloop = nn.CrossEntropyLoss()   \n",
    "optimizer_for_earloop = torch.optim.Adam(model_RNN_for_earloop.parameters(), lr=0.0001)  \n",
    "\n",
    "criterion_for_inarm= nn.CrossEntropyLoss()   \n",
    "optimizer_for_inarm = torch.optim.Adam(model_RNN_for_inarm.parameters(), lr=0.0001)  \n",
    "\n",
    "criterion_for_thumbnail= nn.CrossEntropyLoss()   \n",
    "optimizer_for_thumbnail = torch.optim.Adam(model_RNN_for_thumbnail.parameters(), lr=0.0001)\n",
    "\n",
    "criterion_for_vain = nn.CrossEntropyLoss()   \n",
    "optimizer_for_vain = torch.optim.Adam(model_RNN_for_vain.parameters(), lr=0.0001)  \n",
    "\n",
    "simRNN_models = [model_RNN_for_earloop,model_RNN_for_inarm,model_RNN_for_thumbnail,model_RNN_for_vain]\n",
    "simRNN_criterions = [criterion_for_earloop,criterion_for_inarm,criterion_for_thumbnail,criterion_for_vain]\n",
    "simRNN_optims = [optimizer_for_earloop,optimizer_for_inarm,optimizer_for_thumbnail,optimizer_for_vain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simRNN_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = train_size/batch_size  #for printing purpose\n",
    "num_epochs = 5000\n",
    "\n",
    "train_loss_each_location = []\n",
    "val_loss_each_location = []\n",
    "val_acc_each_location = []\n",
    "\n",
    "for model,criterion,optimizer,each_train_loader,each_val_loader,datasetlabel in zip(simRNN_models,simRNN_criterions,simRNN_optims,train_loader_all,val_loader_all,DATALABEL):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (spec, y) in enumerate(each_train_loader):  \n",
    "            \n",
    "\n",
    "            spec = spec.reshape(spec.shape[0],1,-1).to(device)\n",
    "            y = y.to(device)\n",
    "                    \n",
    "            # Forward pass\n",
    "            outputs = model(spec)\n",
    "            loss = criterion(outputs, y)  \n",
    "        \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i) % 50 == 0:\n",
    "                sys.stdout.write('\\rEpoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                    .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "        if (epoch+1) % 40 == 0:\n",
    "            with torch.no_grad():\n",
    "                total_val_corr = 0\n",
    "                for (val_spec, val_label) in each_val_loader:\n",
    "                    val_spec = val_spec.reshape(val_spec.shape[0],1,-1).to(device)\n",
    "                    val_label = val_label.to(device)\n",
    "                    val_yhat = model(val_spec)\n",
    "                    val_loss = criterion(val_yhat, val_label)\n",
    "\n",
    "                    train_losses.append(loss.cpu().detach().numpy())\n",
    "                    val_losses.append(val_loss.cpu().detach().numpy())\n",
    "\n",
    "                    val_predicted = torch.max(val_yhat, 1)[1]\n",
    "                    total_val_corr += (val_predicted == val_label).sum()\n",
    "                val_acc = (total_val_corr * 100) / val_size\n",
    "                print(f\"    ++++++Validation++++++  Loss: {val_loss:.2f} - Acc: {val_acc:.2f}\",end = ' ')\n",
    "    print(f'\\n END OF MODEL for {datasetlabel} with val acc = {val_acc}')\n",
    "    train_loss_each_location.append(train_losses)\n",
    "    val_loss_each_location.append(val_losses)\n",
    "    val_acc_each_location.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WATCH_PROB_PREDICT:\n",
    "    with torch.no_grad():\n",
    "        print('trainpred ',simRNN_models[-1](spec),'\\n',y,'\\n val\\n',simRNN_models[-1](val_spec),val_label)\n",
    "        print(criterion(simRNN_models[-1](val_spec),val_label))\n",
    "mtp.loss_plot_sub(train_loss_each_location,val_loss_each_location,val_acc_each_location,DATALABEL,'simple_RNN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ai50')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da867d72de60a3e86a2b69a9a7baea090d67382d01a73f765a7401ae7e7cc0f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
